{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2ea2c8-e018-4713-aade-8a01ab3885f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648b5a92-8cbd-45cb-92ba-b718ba629f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example : Attention Map for the tokens of frame 1: \n",
      "tensor([0.6404, 0.2407, 0.1934, 0.7004, 0.0069, 0.3592, 0.4818, 0.6008, 0.9668,\n",
      "        0.1750, 0.6840, 0.6035, 0.8331, 0.1818, 0.7058, 0.1386, 0.7779, 0.7988,\n",
      "        0.0787, 0.4564, 0.0965, 0.7411, 0.7778, 0.1418, 0.1889, 0.4742, 0.2425,\n",
      "        0.8073, 0.6988, 0.4472, 0.2829, 0.2378, 0.2497, 0.9875, 0.6660, 0.6724,\n",
      "        0.7923, 0.0616, 0.1201, 0.4395, 0.1286, 0.2702, 0.8954, 0.7531, 0.5376,\n",
      "        0.7369, 0.1449, 0.9912, 0.9562, 0.8112, 0.4442, 0.5831, 0.4216, 0.1040,\n",
      "        0.2171, 0.1679, 0.3076, 0.9386, 0.9061, 0.6639, 0.8172, 0.9311, 0.6379,\n",
      "        0.2627, 0.2324, 0.9860, 0.1023, 0.0500, 0.0907, 0.6694, 0.5931, 0.4810,\n",
      "        0.2235, 0.6389, 0.6386, 0.5263, 0.3426, 0.2549, 0.2487, 0.7174, 0.2559,\n",
      "        0.8263, 0.7175, 0.2918, 0.8234, 0.8742, 0.2673, 0.9093, 0.3831, 0.7890,\n",
      "        0.0109, 0.5015, 0.7419, 0.2233, 0.7751, 0.7962, 0.2987, 0.2038, 0.2066,\n",
      "        0.3175, 0.5388, 0.1610, 0.6415, 0.8088, 0.4262, 0.7893, 0.0823, 0.7492,\n",
      "        0.1520, 0.2679, 0.9158, 0.0559, 0.2573, 0.3924, 0.1574, 0.0911, 0.8884,\n",
      "        0.4761, 0.7965, 0.6941, 0.8551, 0.7043, 0.5837, 0.2557, 0.1419, 0.0760,\n",
      "        0.8095, 0.8202, 0.7322, 0.2639, 0.5397, 0.3025, 0.6881, 0.5753, 0.9174,\n",
      "        0.6054, 0.6817, 0.1237, 0.5058, 0.5807, 0.2555, 0.0678, 0.7354, 0.6862,\n",
      "        0.1483, 0.4311, 0.8656, 0.3281, 0.3314, 0.6906, 0.4395, 0.1601, 0.3035,\n",
      "        0.1493, 0.4354, 0.3045, 0.2652, 0.5211, 0.1520, 0.6174, 0.0219, 0.8941,\n",
      "        0.6748, 0.9272, 0.3694, 0.8252, 0.1859, 0.9486, 0.0769, 0.3809, 0.1488,\n",
      "        0.5962, 0.4292, 0.7091, 0.0722, 0.1931, 0.4423, 0.2390, 0.0651, 0.4666,\n",
      "        0.1362, 0.9577, 0.4250, 0.1649, 0.2125, 0.7846, 0.2551, 0.4246, 0.0819,\n",
      "        0.2359, 0.3157, 0.7609, 0.8763, 0.3299, 0.8906, 0.8069])\n",
      "\n",
      "Number of visible tokens per frame: 49\n"
     ]
    }
   ],
   "source": [
    "# Simulate the inputs\n",
    "B = 1 # Batch size\n",
    "T = 16 # Number of frames\n",
    "N = 14*14 # Number of tokens per frame\n",
    "mask_ratio = 0.75\n",
    "\n",
    "BT = B*T\n",
    "attn = torch.rand(BT, N) # Simulated Attention Map showing the score of each token in each frame across the batch\n",
    "print(f\"Example : Attention Map for the tokens of frame 1: \\n{attn[0]}\")\n",
    "\n",
    "# Step 1 : Calculate the number of visible patches\n",
    "N_vis = N - int(N * mask_ratio) # Number of visible tokens based on mask ratio\n",
    "print(f\"\\nNumber of visible tokens per frame: {N_vis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f70fc7bf-7aaf-4637-92f8-b67c44396b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled token indices shape: (shape: torch.Size([16, 196])):\n",
      "Importance: tensor([[120,  76, 185,  ..., 168,  46,  90],\n",
      "        [ 88, 183, 189,  ..., 113, 154, 165],\n",
      "        [152, 127, 184,  ..., 183,  85,  16],\n",
      "        ...,\n",
      "        [154, 112,  77,  ..., 103,  26, 192],\n",
      "        [119,  76, 111,  ..., 147,  37,  20],\n",
      "        [161,  72,   6,  ..., 163, 107, 112]])\n",
      "\n",
      "Example: Importance for the tokens of frame 1: \n",
      "tensor([120,  76, 185, 110,   5,  94, 121,  82,  45, 176, 128, 192,  48, 105,\n",
      "         12, 191, 136, 152,  10, 144, 135,  35,  93,   3, 154, 182, 167, 107,\n",
      "          8, 143,  49,  86,  22,   0, 195, 133, 118,  84,  79,  71, 147,  28,\n",
      "        172,  69,  61,  62,  34,  33,  83, 104,  63, 119,  58, 181, 126, 194,\n",
      "         53, 180, 148,  54,  36,  65, 103,  41, 116,  89, 157,  56,  92,  51,\n",
      "          7, 134,  17, 156, 123,  87,  16, 186, 165, 102,  57,  30, 140, 174,\n",
      "        150,  42, 179,  43, 161,  59,  50,  77, 138,  72, 171, 122, 187, 170,\n",
      "         21,  74, 146,  81, 190,  44, 101, 127,  64, 163, 178,  14,  11,  85,\n",
      "        175,  27, 124,  31,  91,  88,  95,  47,   9, 169, 131, 132, 100,  52,\n",
      "         78,  68, 149,  70,  20,  80,   2, 130, 159, 113, 173,  25,  32, 188,\n",
      "        117, 189,  24, 158,  60,  75,  39,  73,  15, 142, 183,  26, 162,  99,\n",
      "         55, 153,  13,   1,  19, 114,  40, 177, 106, 112,  29,   6, 164, 160,\n",
      "         38, 109,  18, 111,  23, 139, 145, 129, 166,  97, 193, 151,  67,  66,\n",
      "        141,  98, 108,   4, 155, 125, 115,  96, 184,  37, 137, 168,  46,  90])\n"
     ]
    }
   ],
   "source": [
    "# Step 2 : Sample tokens based on attention scores\n",
    "importance = torch.multinomial(attn, N) # Sample all tokens based on attention scores\n",
    "print(f\"Sampled token indices shape: (shape: {importance.shape}):\\nImportance: {importance}\")\n",
    "print(f\"\\nExample: Importance for the tokens of frame 1: \\n{importance[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b9f0f5-6c12-4472-b2c0-080b9506ba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized mask: tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize the mask (all tokens are masked initially)\n",
    "bool_masked_pos = torch.ones((BT, N), dtype=torch.bool)  # All masked (True)\n",
    "print(f\"Initialized mask: {bool_masked_pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e296f277-d260-4fe9-aa3f-b9910f6d6b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We define a tensor filled with 0 for frame nb°0 having N_vis elements:\n",
      " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "We define a tensor filled with the first N_vis elemenets in importance:\n",
      " tensor([120,  76, 185, 110,   5,  94, 121,  82,  45, 176, 128, 192,  48, 105,\n",
      "         12, 191, 136, 152,  10, 144, 135,  35,  93,   3, 154, 182, 167, 107,\n",
      "          8, 143,  49,  86,  22,   0, 195, 133, 118,  84,  79,  71, 147,  28,\n",
      "        172,  69,  61,  62,  34,  33,  83])\n",
      "The visible & masked tokens in Frame 0: \n",
      " tensor([False,  True,  True, False,  True, False,  True,  True, False,  True,\n",
      "        False,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True,  True, False,  True,\n",
      "         True,  True,  True, False, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True, False,  True,  True, False, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True, False, False,  True,  True,  True,  True,  True,  True, False,\n",
      "         True, False,  True,  True,  True,  True, False,  True,  True, False,\n",
      "         True,  True, False, False, False,  True, False,  True,  True,  True,\n",
      "         True,  True,  True, False, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True, False,  True, False,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "        False, False,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "         True,  True,  True, False,  True, False, False,  True,  True,  True,\n",
      "         True,  True,  True, False, False,  True,  True, False,  True,  True,\n",
      "         True,  True, False,  True, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
      "         True,  True, False,  True,  True,  True, False,  True,  True,  True,\n",
      "         True,  True, False,  True,  True, False,  True,  True,  True,  True,\n",
      "         True, False, False,  True,  True, False])\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Mark visible tokens in the mask\n",
    "pos1 = torch.arange(BT).view(-1, 1).repeat(1, N_vis)  # Total frames indices for visible tokens\n",
    "print(f\"We define a tensor filled with 0 for frame nb°0 having N_vis elements:\\n\", pos1[0])\n",
    "pos2 = importance[:, :N_vis]  # Indices of selected visible tokens\n",
    "print(f\"We define a tensor filled with the first N_vis elemenets in importance:\\n\",pos2[0])\n",
    "bool_masked_pos[pos1, pos2] = False  # Set visible tokens to unmasked (False)\n",
    "print(f\"The visible & masked tokens in Frame 0: \\n\", bool_masked_pos[0]) # -> Example : Tokens 120, 76, 0 etc belong to the first N_vis elements in importance so they are marked with False -> visible "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
